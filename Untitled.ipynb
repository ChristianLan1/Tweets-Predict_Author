{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to read data files,time:16:25:46\n",
      "File Reading Finish,time:16:25:55\n"
     ]
    }
   ],
   "source": [
    "from test_set_read import Read_from_TestSet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "train_data = Read_from_TestSet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import train_test_split\\nauthor = df[\\'author\\']\\ntext = df[\\'text\\']\\nX_train, X_test, Y_train, Y_test = train_test_split(author, text)\\nprint(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "res = defaultdict(list) \n",
    "for i, j in train_data: \n",
    "    res[i] = res[i]+ [x for x in j]\n",
    "my_dict = dict(res)\n",
    "#print(my_dict['8746'])\n",
    "#print(dct)\n",
    "print(my_dict['8746'].count('.....'))\n",
    "\"\"\"df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "author = 0\n",
    "text_list = []\n",
    "text = {}\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    #print(row['author'])\n",
    "    if row['author'] in text:\n",
    "        \n",
    "        \n",
    "        text[row['author']].extend(row['text'])\n",
    "        \n",
    "        \n",
    "       \n",
    "    else:\n",
    "        \n",
    "        \n",
    "        text[row['author']] = row['text']\n",
    "       \n",
    "print(text['8746'])\"\"\"\n",
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "author = df['author']\n",
    "text = df['text']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(author, text)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('train_tweets.txt', header = None,sep='\\t')\n",
    "#print(df)\n",
    "df.columns = ['author','text']\n",
    "#df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "author = df['author']\n",
    "text = df['text']\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, author)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['Breaking', 'Bad', 'Season', '2:', 'http://tinyurl.com/b6doex'], tags=['Train_0']),\n",
       " TaggedDocument(words=['Kevin', 'Killen,', 'Mixing', 'has', 'just', 'logged', 'into', 'Virtual', 'Glass.'], tags=['Train_1']),\n",
       " TaggedDocument(words=['@handle', 'I', \"didn't\", 'even', 'have', 'to', 'follow', 'the', 'link', 'to', 'get', 'it.', 'Me', '=', 'geek.'], tags=['Train_2'])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 328195/328195 [00:00<00:00, 1298267.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 328195/328195 [00:00<00:00, 1407235.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 328195/328195 [00:00<00:00, 1093062.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 328195/328195 [00:00<00:00, 1118924.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 328195/328195 [00:00<00:00, 1435842.73it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 328195/328195 [00:00<00:00, 1353779.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=30, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 30, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 30, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#mlb = MultiLabelBinarizer()\n",
    "#mlb.fit_transform(y_train)\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
