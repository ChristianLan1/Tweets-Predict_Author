{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               test_Data\n",
      "0      [Some, people, say, that, rappers, don, â€™, t, ...\n",
      "1      [Do, you, know, how, to, tweet, on, a, Blackbe...\n",
      "2      [\", Yoga, is, the, cessation, of, mind, .\", -,...\n",
      "3      [@, handle, Well, ,, with, my, millions, of, d...\n",
      "4      [Cambria, hotels, free, guide, http, ://, hote...\n",
      "5      [May, the, force, of, Jesus, be, with, you, ht...\n",
      "6                [YEAH, !, It, ', s, finally, Monday, !]\n",
      "7      [Martin, Laird, won, in, Las, Vegas, last, wee...\n",
      "8      [Joe, ', s, Crab, Shack, Fundraiser, benefitti...\n",
      "9      [i, hate, my, self, -, portrait, painting, ,, ...\n",
      "10     [FREE, system, to, generate, leads, !, http, :...\n",
      "11     [Life, is, tough, ., But, Jesus, softens, it, ...\n",
      "12     [@, handle, What, is, really, tragic, is, the,...\n",
      "13     [@, handle, Christine, Lahti, ,, she, is, goin...\n",
      "14     [RT, @, handle, :, @, handle, Does, Twitter, s...\n",
      "15     [hello, my, lovely, ', s, .., let, ', s, get, ...\n",
      "16     [New, Blog, Post, !, @, handle, ', s, top, fas...\n",
      "17     [Control, your, diet, by, drinking, plenty, of...\n",
      "18     [rt, @, handle, 20, breathtaking, realistic, C...\n",
      "19     [@, handle, only, plays, german, speed, techno...\n",
      "20     [@, handle, yes, please, !!!, what, ', s, your...\n",
      "21                                [Hey, chic, @, handle]\n",
      "22     [CleanTech, ', s, Ten, Clean, Technology, Pred...\n",
      "23     [5, of, the, Best, Free, and, Open, Source, CD...\n",
      "24     [Check, out, the, November, edition, of, Facto...\n",
      "25     [lmfao, !, RT, @, handle, @, handle, yea, but,...\n",
      "26     [RT, @, handle, \", Your, future, is, spotless,...\n",
      "27     [#, Ad, Really, good, site, for, community, bu...\n",
      "28           [Love, getting, my, glow, on, in, miami, !]\n",
      "29     [Hardesty, ', s, spin, move, =, badass, ., Giv...\n",
      "...                                                  ...\n",
      "35407  [RT, @, handle, Confrontation, leads, to, solu...\n",
      "35408  [\", no, talking, rule, \", in, effect, for, the...\n",
      "35409                           [@, handle, Will, do, !]\n",
      "35410  [2034, Bobwhite, Lane, ,, Santa, Cruz, ,, CA, ...\n",
      "35411  [@, handle, I, think, there, ', s, a, shot, .,...\n",
      "35412  [Stand, Up, To, Cancer, !!, Andrew, McMahon, o...\n",
      "35413   [@, handle, You, are, very, welcome, ,, Mark, .]\n",
      "35414  [Vegas, stereotype, :, 6, Asian, girls, stumbl...\n",
      "35415     [Red, ,, red, wine, ., Stay, close, to, me, .]\n",
      "35416  [Should, health, care, workers, be, required, ...\n",
      "35417  [just, joined, a, video, chat, with, 2, other,...\n",
      "35418  [Celebrated, 30, days, sobriety, n, 10, pounds...\n",
      "35419  [@, handle, I, never, read, the, book, !, They...\n",
      "35420  [I, just, got, an, email, from, an, affiliate,...\n",
      "35421  [Meeting, hubby, for, lunch, -, unexpected, su...\n",
      "35422                                  [@, handle, LMAO]\n",
      "35423  [@, handle, yess, OOO, !, I, ', m, back, in, t...\n",
      "35424  [Spent, the, afternoon, taking, care, of, a, s...\n",
      "35425           [searching, for, social, media, experts]\n",
      "35426  [The, results, are, in, ..., see, who, won, th...\n",
      "35427  [RT, @, handle, :, @, handle, It, always, amaz...\n",
      "35428  [How, To, Get, A, Google, Wave, Account, http,...\n",
      "35429  [Think, spring, :, It, ', s, time, to, plan, y...\n",
      "35430  [Working, in, the, villages, tonight, ., Text,...\n",
      "35431  [omg, ,, Jimmy, Choo, is, coming, out, with, a...\n",
      "35432  [NCAA, Football, Odds, -, West, Virginia, at, ...\n",
      "35433  [@, handle, My, favorite, color, !!, Congrats, !]\n",
      "35434  [It, ', s, not, a, simple, matter, to, let, ot...\n",
      "35435  [Funny, ., The, \", Smoke, N, Drank, \", track, ...\n",
      "35436                           [@, handle, Sup, hun, ?]\n",
      "\n",
      "[35437 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from test_set_read import Read_from_TestSet,Read_from_test_set_unlabelled\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "#train_data = Read_from_TestSet()\n",
    "#df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "test_data = Read_from_test_set_unlabelled()\n",
    "\n",
    "df = pd.DataFrame({'test_Data':test_data})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328932, 2)\n",
      "Training set has 246699 instances. Test set has 82233 instances.\n"
     ]
    }
   ],
   "source": [
    "#df.reset_index(drop=True, inplace=True)\n",
    "#dt=np.dtype('int,string')\n",
    "train_data = np.array(train_data)\n",
    "#author\n",
    "df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "print(df.shape)\n",
    "author = df['author']\n",
    "text = df['text']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text, author)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                             tokenizer=lambda doc: doc, lowercase=False,\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "training_features = vectorizer.fit_transform(X_train)    \n",
    "test_features = vectorizer.transform(X_test)\n",
    "#Y_train = mlb.fit_transform(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_train1 = mlb.fit_transform(Y_train)\n",
    "classifier = OneVsRestClassifier(LogisticRegression(n_jobs=1, C=1e5))\n",
    "classifier.fit(training_features, Y_train1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "Y_test1 = mlb.transform(Y_test)\n",
    "#print(Y_test1)\n",
    "Y_test_pred = classifier.predict(test_features)\n",
    "#print(Y_test_pred)\n",
    "print(accuracy_score(Y_test1, Y_test_pred))\n",
    "#from skmultilearn.problem_transform import LabelPowerset\n",
    "#classifier = LabelPowerset(LogisticRegression(C=1,solver = 'lbfgs'))\n",
    "#classifier.fit(X_train, y_train)\n",
    "\"\"\"print(Y_train.shape)\n",
    "clf = LogisticRegression(C=1,solver = 'lbfgs')#C is the inverse of lambda\n",
    "#MultiLabelBinarizer().fit_transform(Y_train)\n",
    "for i in range(training_features.shape[0]):\n",
    "    if(i == 0): \n",
    "       continue\n",
    "    clf.fit(training_features[:i], Y_train[:i])\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "Y_train = mlb.fit_transform(Y_train)\n",
    "print(Y_train)\n",
    "clf.fit(training_features, Y_train)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
