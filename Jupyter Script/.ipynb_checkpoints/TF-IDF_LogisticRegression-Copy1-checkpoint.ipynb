{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to read data files,time:20:06:23\n",
      "File Reading Finish,time:20:06:28\n"
     ]
    }
   ],
   "source": [
    "from test_set_read import Read_from_TestSet,Read_from_test_set_unlabelled\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "train_data = Read_from_TestSet()\n",
    "#df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "test_data = Read_from_test_set_unlabelled()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8746'\n",
      "  list(['@', 'handle', 'Let', '', 's', 'try', 'and', 'catch', 'up', 'live', 'next', 'week', '!'])]\n",
      " ['8746'\n",
      "  list(['Going', 'to', 'watch', 'Grey', '', 's', 'on', 'the', 'big', 'screen', '-', 'Thursday', 'indulgence', '.'])]\n",
      " ['8746'\n",
      "  list(['@', 'handle', 'My', 'pleasure', 'Patrick', '.', 'hope', 'you', 'are', 'well', '!'])]\n",
      " ...\n",
      " ['4357'\n",
      "  list(['TimeSight', 'Systems', '™', 'Announces', 'Next', '-', 'Generation', 'Platform', 'for', 'Intelligent', 'Network', 'Video', 'Recorders', 'w', '.', 'http', '', 'bit', '.', 'ly', '', '1Dkuwf', '', 'tradeshow'])]\n",
      " ['4357'\n",
      "  list(['Diebold', 'Makes', 'Its', 'Leading', 'Monitoring', 'Solutions', 'Available', 'to', 'Dealers', 'http', '', 'bit', '.', 'ly', '', '3yUDpM', '', 'tradeshow'])]\n",
      " ['4357'\n",
      "  list(['GVI', 'Security', 'Solutions', 'to', 'Introduce', 'AutoIP', '™', 'VMS', 'and', 'Industry', 'Leading', 'Samsung', 'Electronics', '43X', 'Zo', '.', 'http', '', 'bit', '.', 'ly', '', 'GpUa6', '', 'tradeshow'])]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328932, 2)\n",
      "0         [@, handle, Let, , s, try, and, catch, up, liv...\n",
      "1         [Going, to, watch, Grey, , s, on, the, big, sc...\n",
      "2         [@, handle, My, pleasure, Patrick, ., hope, yo...\n",
      "3         [@, handle, Hi, there, !, Been, traveling, a, ...\n",
      "4         [RT, @, handle, Looking, to, Drink, Clean, &, ...\n",
      "5         [RT, @, handle, , Ft, ., Hood, official, confi...\n",
      "6         [RT, @, handle, , Mickey, Mouse, is, Getting, ...\n",
      "7         [@, handle, How, did, u, get, the, invite, Jus...\n",
      "8         [@, handle, I, think, I, am, still, a, good, f...\n",
      "9         [@, handle, I, remember, !, I, am, fine, -, ho...\n",
      "10        [@, handle, That, , s, great, -, good, for, th...\n",
      "11        [@, handle, I, don, , t, want, to, picture, u,...\n",
      "12        [@, handle, D, -, Thanks, for, the, RTs, ., ar...\n",
      "13        [@, handle, Grrr, ., you, must, be, going, cra...\n",
      "14        [@, handle, Hi, there, -, just, catching, up, ...\n",
      "15        [RT, @, handle, , If, you, , re, looking, for,...\n",
      "16        [RT, @, handle, , Retailers, who, aren, ’, t, ...\n",
      "17        [RT, @, handle, , Director, of, Global, Brand,...\n",
      "18        [Still, in, car, ., want, to, jump, out, ., 45...\n",
      "19        [RT, @, handle, , , Only, surround, yourself, ...\n",
      "20        [@, handle, wish, I, could, but, 24, , 7, w, s...\n",
      "21        [RT, @, handle, , Help, u, help, MusiCares, !,...\n",
      "22                   [@, handle, yum, !, Save, me, some, !]\n",
      "23        [RT, @, handle, , Gratitude, is, the, sign, of...\n",
      "24        [@, handle, I, don, , t, think, I, know, what,...\n",
      "25        [RT, @, handle, , @, handle, Just, found, you,...\n",
      "26        [RT, @, handle, , RT, @, handle, , Travelling,...\n",
      "27        [Just, entering, ohio, -, special, hi, to, @, ...\n",
      "28        [@, handle, well, we, agree, on, one, food, th...\n",
      "29                                  [@, handle, only, 1, !]\n",
      "                                ...                        \n",
      "328902                                          [@, handle]\n",
      "328903    [@, handle, Please, add, me, to, the, , awsms0...\n",
      "328904    [@, handle, great, party, last, night, ., met,...\n",
      "328905    [Alta, Phoenix, Lofts, , 1, Phoenix, !, Congra...\n",
      "328906        [You, manage, thing, ;, you, lead, people, .]\n",
      "328907    [Not, to, know, is, bad, ;, not, to, wish, to,...\n",
      "328908    [That, there, should, one, man, die, ignorant,...\n",
      "328909                 [Will, is, character, in, action, .]\n",
      "328910    [What, the, mind, dwells, upon, , the, body, a...\n",
      "328911    [Success, a, I, see, it, , is, a, result, , no...\n",
      "328912    [All, generalization, are, false, , including,...\n",
      "328913    [It, is, the, province, of, knowledge, to, spe...\n",
      "328914    [A, leader, , once, convinced, that, a, partic...\n",
      "328915    [You, can, not, make, excuse, and, money, at, ...\n",
      "328916    [Henry, Brothers, Electronics, , Inc, ., to, P...\n",
      "328917    [TechInsights, , ESC, UK, Event, Showcases, Le...\n",
      "328918    [DEMOfall, 09, Announces, Lineup, of, Emerging...\n",
      "328919    [AFP, Hosts, Symposium, on, Essentials, for, D...\n",
      "328920    [AlertEnterprise, Wins, ASIS, Accolades, 2009,...\n",
      "328921    [Andrews, International, Introduces, New, Meth...\n",
      "328922    [Innovation, Strong, Despite, Recession, , Hum...\n",
      "328923    [VideoIQ, and, Milestone, Systems, Partner, to...\n",
      "328924    [Phoenix, Technologies, to, Showcase, Cutting,...\n",
      "328925    [AnyDATA, , s, APT, -, 210, Tracking, Device, ...\n",
      "328926    [Samplify, Systems, Announces, Distribution, A...\n",
      "328927    [Steelbox, Demonstrates, Open, Video, Framewor...\n",
      "328928    [Small, Businesses, Rely, on, Sage, to, Help, ...\n",
      "328929    [TimeSight, Systems, ™, Announces, Next, -, Ge...\n",
      "328930    [Diebold, Makes, Its, Leading, Monitoring, Sol...\n",
      "328931    [GVI, Security, Solutions, to, Introduce, Auto...\n",
      "Name: text, Length: 328932, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#df.reset_index(drop=True, inplace=True)\n",
    "#dt=np.dtype('int,string')\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "#author\n",
    "df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "print(df.shape)\n",
    "author = df['author']\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\"\"\"def lemmatize_text(text):\n",
    "    lemmatized_output = ' '.join([wordnet_lemmatizer.lemmatize(w) for w in text])\n",
    "    return lemmatized_output\"\"\"\n",
    "def regular_expression(words):\n",
    "    temp = []\n",
    "    #for words in text:\n",
    "    print(words)\n",
    "    words = re.sub(r'[:|,|)|(|\\|/]','',words)\n",
    "    words = re.sub(r'[\\'|\"|]','',words)\n",
    "    words = re.sub('!+','!',words)\n",
    "    words = re.sub(r'\\.+',r'.',words)\n",
    "    words = re.sub(r'\\$+',r'$',words)\n",
    "    words = re.sub(r'\\*+',r'*',words)\n",
    "    words=words.strip()\n",
    "    temp.append(words)\n",
    "    print(temp)\n",
    "    lemmatized_output = ' '.join([wordnet_lemmatizer.lemmatize(w) for w in temp])\n",
    "    \n",
    "    return lemmatized_output\n",
    "\n",
    "\"\"\"df=df['text'].apply(regular_expression)\n",
    "print(\"after regex\",df['text'][:10])\n",
    "df=df['text'].apply(lemmatize_text)\n",
    "print(\"after lemma\",df['text'][:10])\"\"\"\n",
    "text = df['text']\n",
    "#text = df['text'].apply(lemmatize_text)\n",
    "\"\"\"for lists in df['text'].iteritmes():\n",
    "    for words in lists:\n",
    "        words = words.strip()\n",
    "        text.append(words)\"\"\"\n",
    "        \n",
    "\"\"\"for index,list_words in text.iteritems():\n",
    "    for words in list_words:\n",
    "        list_words[words] = porter.stem(words)\n",
    "        wordnet_lemmatizer.lemmatize(words)\"\"\"\n",
    "print(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ handle Let  s try and catch up live next week !\n"
     ]
    }
   ],
   "source": [
    "#print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 263145 instances. Test set has 65787 instances.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text, author,test_size=0.2)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n",
    "#print(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['then']\n",
      "['be']\n",
      "['and']\n",
      "['front']\n",
      "['somewhere']\n",
      "['become']\n",
      "['thus']\n",
      "['each']\n",
      "['next']\n",
      "['see']\n",
      "['interest']\n",
      "['together']\n",
      "['whole']\n",
      "['still']\n",
      "['anyone']\n",
      "['between']\n",
      "['would']\n",
      "['couldnt']\n",
      "['among']\n",
      "['us']\n",
      "['many']\n",
      "['throughout']\n",
      "['except']\n",
      "['either']\n",
      "['through']\n",
      "['show']\n",
      "['seemed']\n",
      "['which']\n",
      "['back']\n",
      "['this']\n",
      "['became']\n",
      "['every']\n",
      "['an']\n",
      "['if']\n",
      "['inc']\n",
      "['anyhow']\n",
      "['only']\n",
      "['therein']\n",
      "['etc']\n",
      "['own']\n",
      "['somehow']\n",
      "['cannot']\n",
      "['eg']\n",
      "['from']\n",
      "['whence']\n",
      "['you']\n",
      "['where']\n",
      "['whenever']\n",
      "['seems']\n",
      "['what']\n",
      "['former']\n",
      "['am']\n",
      "['hence']\n",
      "['both']\n",
      "['much']\n",
      "['were']\n",
      "['themselves']\n",
      "['full']\n",
      "['anything']\n",
      "['whereas']\n",
      "['are']\n",
      "['nothing']\n",
      "['get']\n",
      "['becoming']\n",
      "['bill']\n",
      "['since']\n",
      "['latter']\n",
      "['top']\n",
      "['the']\n",
      "['found']\n",
      "['first']\n",
      "['herself']\n",
      "['sincere']\n",
      "['because']\n",
      "['when']\n",
      "['whither']\n",
      "['everywhere']\n",
      "['can']\n",
      "['five']\n",
      "['empty']\n",
      "['six']\n",
      "['part']\n",
      "['twenty']\n",
      "['myself']\n",
      "['whereupon']\n",
      "['below']\n",
      "['towards']\n",
      "['fill']\n",
      "['nobody']\n",
      "['might']\n",
      "['re']\n",
      "['two']\n",
      "['very']\n",
      "['though']\n",
      "['last']\n",
      "['less']\n",
      "['bottom']\n",
      "['our']\n",
      "['by']\n",
      "['hereby']\n",
      "['move']\n",
      "['now']\n",
      "['along']\n",
      "['off']\n",
      "['who']\n",
      "['being']\n",
      "['amoungst']\n",
      "['meanwhile']\n",
      "['he']\n",
      "['amount']\n",
      "['all']\n",
      "['had']\n",
      "['against']\n",
      "['give']\n",
      "['hereafter']\n",
      "['least']\n",
      "['rather']\n",
      "['everything']\n",
      "['whatever']\n",
      "['thereby']\n",
      "['hers']\n",
      "['ten']\n",
      "['no']\n",
      "['hereupon']\n",
      "['hasnt']\n",
      "['could']\n",
      "['thru']\n",
      "['con']\n",
      "['third']\n",
      "['anywhere']\n",
      "['they']\n",
      "['hundred']\n",
      "['whoever']\n",
      "['go']\n",
      "['down']\n",
      "['alone']\n",
      "['for']\n",
      "['beyond']\n",
      "['cry']\n",
      "['ltd']\n",
      "['eight']\n",
      "['anyway']\n",
      "['de']\n",
      "['it']\n",
      "['she']\n",
      "['yet']\n",
      "['call']\n",
      "['ours']\n",
      "['ourselves']\n",
      "['fifty']\n",
      "['himself']\n",
      "['latterly']\n",
      "['wherein']\n",
      "['not']\n",
      "['whom']\n",
      "['behind']\n",
      "['one']\n",
      "['namely']\n",
      "['than']\n",
      "['mine']\n",
      "['beforehand']\n",
      "['name']\n",
      "['herein']\n",
      "['well']\n",
      "['please']\n",
      "['been']\n",
      "['your']\n",
      "['itself']\n",
      "['me']\n",
      "['further']\n",
      "['thence']\n",
      "['noone']\n",
      "['none']\n",
      "['across']\n",
      "['someone']\n",
      "['these']\n",
      "['three']\n",
      "['co']\n",
      "['elsewhere']\n",
      "['without']\n",
      "['his']\n",
      "['whereafter']\n",
      "['onto']\n",
      "['therefore']\n",
      "['thick']\n",
      "['forty']\n",
      "['i']\n",
      "['should']\n",
      "['always']\n",
      "['was']\n",
      "['neither']\n",
      "['via']\n",
      "['moreover']\n",
      "['at']\n",
      "['thin']\n",
      "['ie']\n",
      "['perhaps']\n",
      "['on']\n",
      "['again']\n",
      "['indeed']\n",
      "['their']\n",
      "['its']\n",
      "['everyone']\n",
      "['also']\n",
      "['cant']\n",
      "['sometime']\n",
      "['up']\n",
      "['serious']\n",
      "['some']\n",
      "['mostly']\n",
      "['how']\n",
      "['several']\n",
      "['but']\n",
      "['a']\n",
      "['nor']\n",
      "['almost']\n",
      "['put']\n",
      "['too']\n",
      "['nowhere']\n",
      "['those']\n",
      "['around']\n",
      "['so']\n",
      "['upon']\n",
      "['find']\n",
      "['thereupon']\n",
      "['detail']\n",
      "['may']\n",
      "['seem']\n",
      "['out']\n",
      "['un']\n",
      "['of']\n",
      "['that']\n",
      "['yourself']\n",
      "['something']\n",
      "['must']\n",
      "['other']\n",
      "['after']\n",
      "['above']\n",
      "['others']\n",
      "['afterwards']\n",
      "['whereby']\n",
      "['often']\n",
      "['yours']\n",
      "['into']\n",
      "['here']\n",
      "['such']\n",
      "['formerly']\n",
      "['to']\n",
      "['sixty']\n",
      "['why']\n",
      "['however']\n",
      "['do']\n",
      "['becomes']\n",
      "['amongst']\n",
      "['fire']\n",
      "['or']\n",
      "['never']\n",
      "['few']\n",
      "['yourselves']\n",
      "['else']\n",
      "['once']\n",
      "['seeming']\n",
      "['toward']\n",
      "['fifteen']\n",
      "['nevertheless']\n",
      "['wherever']\n",
      "['due']\n",
      "['done']\n",
      "['another']\n",
      "['him']\n",
      "['besides']\n",
      "['over']\n",
      "['have']\n",
      "['will']\n",
      "['per']\n",
      "['describe']\n",
      "['more']\n",
      "['has']\n",
      "['nine']\n",
      "['while']\n",
      "['is']\n",
      "['eleven']\n",
      "['my']\n",
      "['keep']\n",
      "['side']\n",
      "['within']\n",
      "['there']\n",
      "['until']\n",
      "['them']\n",
      "['in']\n",
      "['with']\n",
      "['during']\n",
      "['otherwise']\n",
      "['enough']\n",
      "['twelve']\n",
      "['we']\n",
      "['thereafter']\n",
      "['take']\n",
      "['even']\n",
      "['sometimes']\n",
      "['four']\n",
      "['whose']\n",
      "['system']\n",
      "['already']\n",
      "['made']\n",
      "['as']\n",
      "['any']\n",
      "['although']\n",
      "['her']\n",
      "['ever']\n",
      "['mill']\n",
      "['about']\n",
      "['before']\n",
      "['same']\n",
      "['under']\n",
      "['beside']\n",
      "['most']\n",
      "['whether']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-d5ebcd9de2b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                              ngram_range=(1, 2))\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtraining_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#Y_train = mlb.fit_transform(Y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1650\u001b[0m         \"\"\"\n\u001b[0;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    350\u001b[0m                                                tokenize)\n\u001b[0;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 352\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-a896bfd40611>\u001b[0m in \u001b[0;36mregular_expression\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#for words in text:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[:|,|)|(|\\|/]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[\\'|\"|]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'!+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'!'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "def doc(text):\n",
    "    return text\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                              lowercase=False,\n",
    "                             #tokenizer= doc,\n",
    "                             min_df = 50,\n",
    "                             preprocessor=regular_expression,\n",
    "                             #strip_accents =None,\n",
    "                             #token_pattern = '\\S',\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "training_features = vectorizer.fit_transform(X_train)    \n",
    "test_features = vectorizer.transform(X_test)\n",
    "#Y_train = mlb.fit_transform(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "print(training_features.shape)\n",
    "print(list_feature_name[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done fitting\n",
      "0.3367\n",
      "--- 3685.428197860718 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#logreg = Pipeline([('vect', CountVectorizer()),\n",
    "#                ('tfidf', TfidfTransformer()),\n",
    "#                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "#               ])\n",
    "#mlb = MultiLabelBinarizer()\n",
    "#Y_train1 = mlb.fit_transform(Y_train)\n",
    "import time\n",
    "start_time = time.time()\n",
    "classifier = OneVsRestClassifier(LogisticRegression(n_jobs=-1, C=1, solver = 'lbfgs'))\n",
    "#classifier = LogisticRegression(n_jobs=1, C=1e5)\n",
    "classifier.fit(training_features, Y_train)\n",
    "print(\"done fitting\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "#Y_test1 = mlb.transform(Y_test)\n",
    "#print(Y_test1)\n",
    "Y_test_pred = classifier.predict(test_features)\n",
    "#print(Y_test_pred)\n",
    "print(accuracy_score(Y_test, Y_test_pred))\n",
    "#from skmultilearn.problem_transform import LabelPowerset\n",
    "#classifier = LabelPowerset(LogisticRegression(C=1,solver = 'lbfgs'))\n",
    "#classifier.fit(X_train, y_train)\n",
    "\"\"\"print(Y_train.shape)\n",
    "clf = LogisticRegression(C=1,solver = 'lbfgs')#C is the inverse of lambda\n",
    "#MultiLabelBinarizer().fit_transform(Y_train)\n",
    "for i in range(training_features.shape[0]):\n",
    "    if(i == 0): \n",
    "       continue\n",
    "    clf.fit(training_features[:i], Y_train[:i])\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "Y_train = mlb.fit_transform(Y_train)\n",
    "print(Y_train)\n",
    "clf.fit(training_features, Y_train)\"\"\"\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#df = pd.DataFrame({'test_Data':test_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'test_Data':test_data})\n",
    "test_real_features = vectorizer.transform(df['test_Data'])\n",
    "Y_test_pred_output = classifier.predict(test_real_features)\n",
    "\n",
    "df = pd.DataFrame(Y_test_pred_output,columns=['Predicted'])\n",
    "\n",
    "\n",
    "#df.to_csv('predict_SML_Pro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.index + 1\n",
    "df.index.name = 'Id'\n",
    "df.to_csv('predict_SML_Pro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
