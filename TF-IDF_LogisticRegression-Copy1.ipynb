{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to read data files,time:16:22:42\n",
      "File Reading Finish,time:16:22:49\n"
     ]
    }
   ],
   "source": [
    "from test_set_read import Read_from_TestSet,Read_from_test_set_unlabelled\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "train_data = Read_from_TestSet()\n",
    "#df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "test_data = Read_from_test_set_unlabelled()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328932, 2)\n",
      "0         [@, handle, Let, ', s, try, and, catch, up, li...\n",
      "1         [Going, to, watch, Grey, ', s, on, the, big, s...\n",
      "2         [@, handle, My, pleasure, Patrick, ...., hope,...\n",
      "3         [@, handle, Hi, there, !, Been, traveling, a, ...\n",
      "4         [RT, @, handle, Looking, to, Drink, Clean, &, ...\n",
      "5         [RT, @, handle, :, Ft, ., Hood, official, conf...\n",
      "6         [RT, @, handle, :, Mickey, Mouse, is, Getting,...\n",
      "7         [@, handle, How, did, u, get, the, invite, Jus...\n",
      "8         [@, handle, I, think, I, am, still, a, good, f...\n",
      "9         [@, handle, I, remember, !, I, am, fine, -, ho...\n",
      "10        [@, handle, That, ', s, great, -, good, for, t...\n",
      "11        [@, handle, I, don, ', t, want, to, picture, u...\n",
      "12        [@, handle, D, -, Thanks, for, the, RTs, ....,...\n",
      "13        [@, handle, Grrr, ...., you, must, be, going, ...\n",
      "14        [@, handle, Hi, there, -, just, catching, up, ...\n",
      "15        [RT, @, handle, :, If, you, ', re, looking, fo...\n",
      "16        [RT, @, handle, :, Retailers, who, aren, ’, t,...\n",
      "17        [RT, @, handle, :, Director, of, Global, Brand...\n",
      "18        [Still, in, car, ...., want, to, jump, out, .....\n",
      "19        [RT, @, handle, :, \", Only, surround, yourself...\n",
      "20        [@, handle, wish, I, could, but, 24, /, 7, w, ...\n",
      "21        [RT, @, handle, :, Help, u, help, MusiCares, !...\n",
      "22               [@, handle, yum, !!!!, Save, me, some, !!]\n",
      "23        [RT, @, handle, :, Gratitude, is, the, sign, o...\n",
      "24        [@, handle, I, don, ', t, think, I, know, what...\n",
      "25        [RT, @, handle, :, @, handle, Just, found, you...\n",
      "26        [RT, @, handle, :, RT, @, handle, :, Travellin...\n",
      "27        [Just, entering, ohio, -, special, hi, to, @, ...\n",
      "28        [@, handle, well, we, agree, on, one, food, th...\n",
      "29                              [@, handle, only, 1, !!!!!]\n",
      "                                ...                        \n",
      "328902                                          [@, handle]\n",
      "328903    [@, handle, Please, add, me, to, the, #, awsms...\n",
      "328904    [@, handle, great, party, last, night, ..., me...\n",
      "328905    [Alta, Phoenix, Lofts, #, 1, Phoenix, !!!!!, C...\n",
      "328906        [You, manage, thing, ;, you, lead, people, .]\n",
      "328907    [Not, to, know, is, bad, ;, not, to, wish, to,...\n",
      "328908    [That, there, should, one, man, die, ignorant,...\n",
      "328909                 [Will, is, character, in, action, .]\n",
      "328910    [What, the, mind, dwells, upon, ,, the, body, ...\n",
      "328911    [Success, a, I, see, it, ,, is, a, result, ,, ...\n",
      "328912    [All, generalization, are, false, ,, including...\n",
      "328913    [It, is, the, province, of, knowledge, to, spe...\n",
      "328914    [A, leader, ,, once, convinced, that, a, parti...\n",
      "328915    [You, can, not, make, excuse, and, money, at, ...\n",
      "328916    [Henry, Brothers, Electronics, ,, Inc, ., to, ...\n",
      "328917    [TechInsights, ', ESC, UK, Event, Showcases, L...\n",
      "328918    [DEMOfall, 09, Announces, Lineup, of, Emerging...\n",
      "328919    [AFP, Hosts, Symposium, on, Essentials, for, D...\n",
      "328920    [AlertEnterprise, Wins, ASIS, Accolades, 2009,...\n",
      "328921    [Andrews, International, Introduces, New, Meth...\n",
      "328922    [Innovation, Strong, Despite, Recession, :, Hu...\n",
      "328923    [VideoIQ, and, Milestone, Systems, Partner, to...\n",
      "328924    [Phoenix, Technologies, to, Showcase, Cutting,...\n",
      "328925    [AnyDATA, ', s, APT, -, 210, Tracking, Device,...\n",
      "328926    [Samplify, Systems, Announces, Distribution, A...\n",
      "328927    [Steelbox, Demonstrates, Open, Video, Framewor...\n",
      "328928    [Small, Businesses, Rely, on, Sage, to, Help, ...\n",
      "328929    [TimeSight, Systems, ™, Announces, Next, -, Ge...\n",
      "328930    [Diebold, Makes, Its, Leading, Monitoring, Sol...\n",
      "328931    [GVI, Security, Solutions, to, Introduce, Auto...\n",
      "Name: text, Length: 328932, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#df.reset_index(drop=True, inplace=True)\n",
    "#dt=np.dtype('int,string')\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "#author\n",
    "df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "print(df.shape)\n",
    "author = df['author']\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return [wordnet_lemmatizer.lemmatize(w) for w in text]\n",
    "def regular_expression(text):\n",
    "    temp = []\n",
    "    for words in text:\n",
    "        words = re.sub(r'[:|,|)|(|\\|/]',r'',words)\n",
    "        words = re.sub(r'[\\'|\"|#]',r'',words)\n",
    "        words = re.sub('!+','!',words)\n",
    "        words = re.sub(r'\\.+',r'.',words)\n",
    "        words = re.sub(r'\\$+',r'$',words)\n",
    "        words = re.sub(r'\\*+',r'*',words)\n",
    "        temp.append(words)\n",
    "    return temp\n",
    "#df['text']=df.text.apply(regular_expression)\n",
    "df['text']=df.text.apply(lemmatize_text)\n",
    "\n",
    "text = df['text']\n",
    "\"\"\"for index,list_words in text.iteritems():\n",
    "    for words in list_words:\n",
    "        list_words[words] = porter.stem(words)\n",
    "        wordnet_lemmatizer.lemmatize(words)\"\"\"\n",
    "print(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 2400 instances. Test set has 600 instances.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text[:3000], author[:3000],test_size=0.2)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n",
    "#print(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "def doc(text):\n",
    "    return text\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                              lowercase=False,\n",
    "                             tokenizer= doc,\n",
    "                             #strip_accents =None,\n",
    "                             token_pattern = '\\S',\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "training_features = vectorizer.fit_transform(X_train)    \n",
    "test_features = vectorizer.transform(X_test)\n",
    "#Y_train = mlb.fit_transform(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 30376)\n",
      "['!', '! \"', '! #', '! (', '! (&', '! (:', '! -', '! ...', '! 3035789881', '! 8pm', '! :', '! :)', '! <<', '! =', '! @', '! A', '! All', '! Also', '! At', '! Automagic', '! Awesome', '! Be', '! Been', '! Billy', '! Brand', '! Can', '! Chaos', '! Charlie', '! Check', '! Coventry', '! DAMN', '! Def', '! Dentists', '! Did', '! Do', '! Doesn', '! Enjoy', '! Enter', '! Facebook', '! Fast', '! For', '! Get', '! Glad', '! Go', '! Good', '! Got', '! Gothic', '! Great', '! Grrrrr', '! HA', '! Haha', '! Have', '! He', '! Hope', '! How', '! I', '! IF', '! In', '! Is', '! It', '! Jamie', '! Just', '! Keep', '! Kellie', '! Knock', '! LOL', '! LOVE', '! Learn', '! Learning', '! Lets', '! Live', '! Looking', '! Lowe', '! MIC', '! Mom', '! Music', '! My', '! NOW', '! News', '! Next', '! Novices', '! Now', '! Oh', '! One', '! Online', '! Options', '! Or', '! P', '! PPL', '! People', '! Please', '! Pls', '! Por', '! RT', '! Read', '! Register', '! Royalties', '! See', '! Senator', '! Shout', '! Simply', '! Skipping', '! So', '! Special', '! Tallahassee', '! Thank', '! Thanks', '! The', '! Then', '! There', '! Thinking', '! This', '! Thursday', '! Time', '! Today', '! Too', '! Ud', '! Unless', '! V', '! Via', '! Vote', '! WSJ', '! WTF', '! Watching', '! We', '! What', '! Whee', '! Where', '! Who', '! Why', '! XD', '! YAAAAY', '! YOU', '! Yay', '! You', '! ad', '! add', '! check', '! elin', '! follow', '! glad', '! going', '! got', '! http', '! let', '! link', '! love', '! need', '! nice', '! peep', '! rumor', '! sure', '! thursday', '! www', '!!', '!! #', '!! ---->>', '!! :(', '!! =', '!! CHANNELSIDE', '!! Cough', '!! Flying', '!! Happy', '!! Have', '!! Learn', '!! Make', '!! May', '!! RT', '!! Then', '!! WTF', '!! Whoop', '!! [[', '!! http', '!! need', '!! wa', '!! www', '!!!', '!!! (', '!!! ...', '!!! 11pm', '!!! 2day', '!!! 7pm', '!!! 8pm', '!!! :', '!!! :)', '!!! Badder', '!!! Check', '!!! Got', '!!! Happy', '!!! I', '!!! If', '!!! It', '!!! Its', '!!! Learn', '!!! Then', '!!! Too', '!!! Watch', '!!! Where', '!!! Whoo', \"!!! `'·.¸?¸.·'´`'·.¸?¸.·'´`'·.¸?¸.·'´`'·.¸?¸.·'´`'·.¸?¸.·'´\", '!!! nap', '!!! wed', '!!!!', '!!!! ...', '!!!! @', '!!!! POW', '!!!! Save', '!!!! http', '!!!!!', '!!!!!!!!', '!!\"', '!!\" \"', '!!)', '!!) saw', '!\"', '!\" Gotta', '!\" Lol', '!\" Mary', '!\" Rofl', '!\" answer', '!\".', \"!'\", \"!' lmao\", '!)', '!) It', '!.', '!. A', '!:', '!: @', '!@', '!@ handle', '!]', '!”', '!” Check', '\"', '\" \"', '\" (', '\" (:', '\" -', '\" ...', '\" 12', '\" 1938', '\" 38', '\" 55', '\" 6', '\" 9', '\" :', '\" ?', '\" A', '\" All', '\" Ammonia', '\" BOOM', '\" Best', '\" Biggest', '\" Buy', '\" Cancer', '\" Carnac', '\" Chicago', '\" City', '\" Crazy', '\" DEAL', '\" DEMO', '\" Dismal', '\" Don', '\" Down', '\" Eclipse', '\" El', '\" First', '\" For', '\" Funkstyles', '\" Get', '\" Gets', '\" Gifts', '\" Girl', '\" Girls', '\" Google', '\" Gossip', '\" Green', '\" HDTV', '\" Having', '\" I', '\" Idol', '\" Impact', '\" In', '\" Interview', '\" LBD', '\" La', '\" Love', '\" MIC', '\" Man', '\" Music', '\" NOW', '\" New', '\" OMG', '\" Old', '\" Only', '\" Ozzy', '\" PDX', '\" Panasonic', '\" Performance', '\" Premiere', '\" Pretty', '\" Product', '\" Protect', '\" Puppy', '\" Really', '\" Returning', '\" Sat', '\" Save', '\" Semifinalist', '\" The', '\" To', '\" Tweet', '\" Twilight', '\" We', '\" Web', '\" What', '\" White', '\" Women', '\" World', '\" YEAH', '\" Yap', '\" You', '\" button', '\" cut', '\" darkside', '\" did', '\" don', '\" edit', '\" entheogens', '\" entrepreneur', '\" fehaciente', '\" fidedigno', '\" gente', '\" glug', '\" green', '\" grey', '\" ha', '\" http', '\" instead', '\" joined', '\" let', '\" lunch', '\" maddie', '\" make', '\" martial', '\" merchandise', '\" message', '\" model', '\" mood', '\" moved', '\" okonomiyaki', '\" open', '\" outfit', '\" pant', '\" people', '\" play', '\" pretty', '\" puma', '\" realtive', '\" renew', '\" right', '\" roaring', '\" robot', '\" screen', '\" sharing', '\" start', '\" talent', '\" venture', '\" veraz', '\" video', '\" w', '\" wallah', '\" woman', '\" y', '\" ~', '\"!!!', '\")', '\") http', '\",', '\", lose', '\"-', '\"- http', '\".', '\". Is', '\". It', '\". Set', '\". http', '\"..', '\".. A', '\"...', '\"... Mr', '\"... strong', '\":', '\": http', '\"?', '\"? That', '\"? http', '#', '# 1', '# 10', '# 2ksports', '# 3', '# 6', '# 8217', '# Blackberry', '# Books', '# CSS', '# Cat', '# China', '# Christmas', '# Chrome', '# Cowboys', '# DEMO09', '# Dell', '# Dog', '# Dogs', '# Eddie', '# Escorts', '# FakeAV', '# FollowFriday', '# Gartner', '# Ghettofact', '# Google', '# GreatStarWarsQuotesDuringSex', '# Halloween', '# Happy', '# Holidays', '# Huawei', '# IBM', '# ISCMO', '# Lenovo', '# MayAWeirdHolyManSoilYourWallpaper', '# Microsoft', '# Mobile', '# MusicMonday', '# MySQL', '# NFLgladiatorsofthegridironactionfigure', '# OWASP', '# Obama', '# October', '# OnlyJustChristmasTime', '# PAICP', '# Portland', '# RTW', '# SAP', '# SHSMD', '# Security', '# Seriously', '# SoFlaBlogUp', '# TPL', '# Telecom', '# Thanksgiving', '# TheAmateurs', '# Thinkpad', '# Travel', '# TweetYourPleasure', '# TwitterACritter', '# Vegas', '# Veterans', '# WebSphere', '# Wecoolandallbut', '# WhoDat', '# WiiFit', '# WorldAIDSday', '# adhocparty', '# aintnothinglike', '# anntaylor', '# anntaylorstyle', '# arg', '# arsenio', '# awesome', '# baseball', '# battlefieldbadcompany2', '# beaverbrownband', '# bestnonchalantcatchever', '# blackfriday', '# boo', '# brand', '# brilliantidea', '# business', '# butyay', '# callofdutymodernwarfarereflex', '# cartopiagateway', '# cat', '# charity', '# childsplay', '# christmas', '# classicmoviequotes', '# clip', '# cloud', '# clusterurl', '# cnn', '# commandconquerredalert', '# compromise', '# creepy', '# cruiser', '# crushit', '# dahboardupdate', '# damselmeetsdistress']\n"
     ]
    }
   ],
   "source": [
    "list_feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "print(training_features.shape)\n",
    "print(list_feature_name[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done fitting\n",
      "0.645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "#logreg = Pipeline([('vect', CountVectorizer()),\n",
    "#                ('tfidf', TfidfTransformer()),\n",
    "#                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "#               ])\n",
    "#mlb = MultiLabelBinarizer()\n",
    "#Y_train1 = mlb.fit_transform(Y_train)\n",
    "#classifier = OneVsRestClassifier(LogisticRegression(n_jobs=1, C=1e5))\n",
    "classifier = LogisticRegression(n_jobs=1, C=1e5)\n",
    "classifier.fit(training_features, Y_train)\n",
    "print(\"done fitting\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "#Y_test1 = mlb.transform(Y_test)\n",
    "#print(Y_test1)\n",
    "Y_test_pred = classifier.predict(test_features)\n",
    "#print(Y_test_pred)\n",
    "print(accuracy_score(Y_test, Y_test_pred))\n",
    "#from skmultilearn.problem_transform import LabelPowerset\n",
    "#classifier = LabelPowerset(LogisticRegression(C=1,solver = 'lbfgs'))\n",
    "#classifier.fit(X_train, y_train)\n",
    "\"\"\"print(Y_train.shape)\n",
    "clf = LogisticRegression(C=1,solver = 'lbfgs')#C is the inverse of lambda\n",
    "#MultiLabelBinarizer().fit_transform(Y_train)\n",
    "for i in range(training_features.shape[0]):\n",
    "    if(i == 0): \n",
    "       continue\n",
    "    clf.fit(training_features[:i], Y_train[:i])\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "Y_train = mlb.fit_transform(Y_train)\n",
    "print(Y_train)\n",
    "clf.fit(training_features, Y_train)\"\"\"\n",
    "df = pd.DataFrame({'test_Data':test_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_real_features = vectorizer.transform(df['test_Data'])\n",
    "Y_test_pred_output = classifier.predict(test_real_features)\n",
    "\n",
    "df = pd.DataFrame(Y_test_pred_output,columns=['Predicted'])\n",
    "\n",
    "\n",
    "#df.to_csv('predict_SML_Pro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.index + 1\n",
    "df.index.name = 'Id'\n",
    "df.to_csv('predict_SML_Pro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
