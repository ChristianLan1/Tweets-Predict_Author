{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to read data files,time:23:22:40\n",
      "File Reading Finish,time:23:22:45\n",
      "(328932, 2)\n",
      "Training set has 296038 instances. Test set has 32894 instances.\n"
     ]
    }
   ],
   "source": [
    "from test_set_read import Read_from_TestSet,Read_from_test_set_unlabelled\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "train_data = Read_from_TestSet()\n",
    "train_data = np.array(train_data)\n",
    "    \n",
    "df = pd.DataFrame(train_data, columns=['author', 'text'])\n",
    "print(df.shape)\n",
    "author = df['author']\n",
    "\n",
    "text = df['text']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text, author,test_size=0.001,random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296038, 70233)\n",
      "['', ' ', '  ', '  !', '  #', '  -', '  .', '  10', '  12584', '  140army', '  140mafia', '  2', '  2009', '  21bikes', '  3', '  301', '  6', '  7', '  ?', '  @', '  A', '  D', '  HUFFPO', '  I', '  IHOP', '  RT', '  U', '  act', '  activerain', '  ad', '  adage', '  adjix', '  advertise', '  alpacaroseranch', '  alturl', '  amplify', '  angstro', '  ashevillefm', '  ask', '  audioposes', '  awe', '  aweber', '  b105', '  bbltwt', '  best', '  bestc', '  beta', '  big', '  bigcha', '  bit', '  bizuz', '  bkite', '  blip', '  blog', '  blu', '  bout', '  bt', '  bte', '  budurl', '  buzzit', '  buzzup', '  caltweet', '  cause', '  chilp', '  chir', '  cli', '  click', '  cpnurl', '  craniumfitteds', '  cz', '  d', '  dailybooth', '  dealnay', '  detroitluv', '  did', '  dig', '  digg', '  dlvr', '  doe', '  doing', '  don', '  dwarfurl', '  easyuri', '  eepurl', '  en', '  ezinearticles', '  fa', '  facebook', '  favorite', '  fb', '  ff', '  flic', '  followe', '  followersfree', '  followerstrain', '  free', '  fwix', '  gawker', '  getinandhangon', '  gide', '  girl', '  giving', '  gizmodo', '  going', '  gonna', '  good', '  google', '  goplanit', '  got', '  gowal', '  goyatime', '  great', '  grp', '  guy', '  ha', '  hamiltonradio', '  happn', '  happy', '  having', '  heatfeed', '  help', '  hennahut', '  herbspritz', '  hey', '  hootsuite', '  hotel', '  hotjobs', '  htxt', '  hubpages', '  icio', '  idek', '  ihid', '  ilike', '  ineedgcs', '  j', '  jamesderr', '  jamz963', '  jeffcrumley', '  joanmaejournal', '  jobsurl', '  jr', '  just', '  kiwcfm', '  kl', '  know', '  kotaku', '  lala', '  let', '  life', '  like', '  likwidenergy', '  limelinx', '  lin', '  linkbee', '  little', '  live', '  ll', '  lnk', '  localtweeps', '  lol', '  look', '  looking', '  loopt', '  love', '  loveinwaiting', '  lunch', '  m', '  make', '  man', '  maplewoodmuse', '  markalandooley', '  mashable', '  mboland', '  mixonian', '  moby', '  moderatemoyer', '  movie', '  mrtweet', '  myfonts', '  myincomeautopilotblog', '  myloc', '  mypict', '  myspace', '  naxnews', '  need', '  networkedblogs', '  new', '  news', '  obama', '  oh', '  ohurl', '  old', '  om', '  online', '  oohja', '  ottawa', '  ow', '  p', '  people', '  photoboxr', '  pic', '  ping', '  pitch', '  pk', '  plurk', '  post', '  preview', '  probably', '  qrl', '  quicktipstoday', '  raptr', '  really', '  reframeit', '  regator', '  retwt', '  revtwt', '  right', '  rly', '  s', '  s1z', '  s2z', '  s3z', '  s4z', '  sandiego', '  say', '  sbnation', '  sbne', '  schmap', '  screenr', '  sexy', '  shar', '  shorebizusa', '  short', '  shorten', '  shrinkify', '  sml', '  sn', '  snipr', '  snipurl', '  snurl', '  social', '  socialmedia', '  somead', '  song', '  sound', '  st', '  startuphire', '  stickam', '  su', '  t', '  texarkanagop', '  thanks', '  thenaughtypenguin', '  theonlineretirement', '  thing', '  think', '  time', '  time4u2chews', '  tinalifford', '  tinify', '  tiny', '  tiny12', '  tinychat', '  tinypaste', '  tinysong', '  tinyurl', '  tl', '  tlre', '  tmogul', '  tobtr', '  tqpage', '  tr', '  trakim', '  trcb', '  trim', '  trunc', '  tumblr', '  tw0', '  twa', '  tweetglide', '  tweetmysong', '  tweetphoto', '  tweetreel', '  twellow', '  twi', '  twibbon', '  twibes', '  twit', '  twitcam', '  twitcause', '  twitdoc', '  twitgoo', '  twitpic', '  twitpwr', '  twitrobot', '  twittascope', '  twitter', '  twittercounter', '  twitthis', '  twiturm', '  twitvid', '  twt', '  twtpoll', '  twtvite', '  twubs', '  twurl', '  txtpros', '  u', '  url4', '  urlpass', '  usat', '  usershare', '  ustre', '  ve', '  video', '  viigo', '  vimeo', '  vogjin', '  voice', '  vote3oh3', '  vur', '  wa', '  want', '  way', '  wee', '  wefollow', '  welcome', '  whbsolutions', '  woo', '  wordstream', '  work', '  world', '  wow', '  wp', '  wpo', '  www', '  www1', '  x', '  xr', '  xrl', '  yfrog', '  youtube', '  yttwt', '  yumurl', '  znl', ' !', ' ! ', ' ! #', ' ! -', ' ! I', ' ! RT', ' ! wed', ' #', ' # 1', ' # 2', ' # 2012', ' # 3', ' # 4', ' # FF', ' # MM', ' # NFL', ' # NORML', ' # churchillclub', ' # entrepreneur', ' # fb', ' # ff', ' # followfriday', ' # fortunecookies', ' # hiring', ' # job', ' # leapfish', ' # music_country', ' # music_pop', ' # musicmonday', ' # opensource', ' # pandora', ' # quote', ' # savannah', ' # smallbiz', ' # tcot', ' # whatsbetter', ' $', ' $ 0', ' $ 1', ' $ 10', ' $ 100', ' $ 15', ' $ 2', ' $ 20', ' $ 25', ' $ 2500', ' $ 3', ' $ 30', ' $ 4', ' $ 5', ' $ 50', ' $ 500', ' &', ' & ', ' & .', ' & @', ' & I', ' & lt', ' &#', ' &# 39', ' *', ' +', ' -', ' - ', ' - #', ' - -', ' - @', ' - A', ' - I', ' - RT', ' - download', ' - new', ' - time', ' - watch', ' --', ' -- ', ' .', ' . ', ' . #', ' . $', ' . -', ' . .', ' . ;', ' . @', ' . I', ' . RT', ' . don', ' . e', ' . gd', ' . good', ' . htm', ' . html', ' . imgur', ' . just', ' . really', ' . wa', ' . ~', ' .-', ' .- new', ' .?', ' .@', ' .@ handle', ' 0', ' 0 ', ' 0 .', ' 0 bid', ' 00', ' 00 ', ' 00 !', ' 00 -', ' 00 .', ' 00 AM', ' 00 PM', ' 00 dry', ' 00 p', ' 00 pm', ' 000', ' 000 ', ' 000 !', ' 000 +', ' 000 -', ' 000 .', ' 000 day', ' 000 follower', ' 000 month', ' 000 new', ' 000 people', ' 000 veterens', ' 000 word', ' 000th', ' 00AM', ' 00PM', ' 00PM ', ' 00am', ' 00pm', ' 00pm ', ' 00pm .', ' 01', ' 01 ', ' 02', ' 02 ', ' 03', ' 03 ', ' 04', ' 04 ', ' 05', ' 05 ', ' 05 -', ' 06', ' 06 ', ' 06c26', ' 07', ' 07 ', ' 08', ' 08 ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def doc(text):\n",
    "    return text\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\"\"\"def lemmatize_text(text):\n",
    "    lemmatized_output = ' '.join([wordnet_lemmatizer.lemmatize(w) for w in text])\n",
    "    return lemmatized_output\"\"\"\n",
    "def regular_expression(words):\n",
    "    temp = []\n",
    "    #for words in text:\n",
    "    if isinstance(words,str):\n",
    "        words = re.sub(r'[:|,|)|(|\\|/]','',words)\n",
    "        words = re.sub(r'[\\'|\"|]','',words)\n",
    "        words = re.sub('!+','!',words)\n",
    "        words = re.sub(r'\\.+',r'.',words)\n",
    "        words = re.sub(r'\\$+',r'$',words)\n",
    "        words = re.sub(r'\\*+',r'*',words)\n",
    "        words = words.replace(\"http\",\"\")\n",
    "        words=words.strip()\n",
    "        if not words.isupper():\n",
    "            words = words.lower()\n",
    "            #print(\"single words\",words)\n",
    "        return [wordnet_lemmatizer.lemmatize(words)] \n",
    "    else:\n",
    "        for word in words:\n",
    "            word = re.sub(r'[:|,|)|(|\\|/]','',word)\n",
    "            word = re.sub(r'[\\'|\"|]','',word)\n",
    "            word = re.sub('!+','!',word)\n",
    "            word = re.sub(r'\\.+',r'.',word)\n",
    "            word = re.sub(r'\\$+',r'$',word)\n",
    "            word = re.sub(r'\\*+',r'*',word)\n",
    "            word = word.replace(\"http\",\"\")\n",
    "            word=word.strip()\n",
    "            if not word.isupper():\n",
    "                #print(word)\n",
    "                word = word.lower()\n",
    "            temp.append(word)\n",
    "        words = temp\n",
    "    #lemmatized_output = words\n",
    "    lemmatized_output = [wordnet_lemmatizer.lemmatize(w) for w in words]\n",
    "    lemmatized_output.append(str(len(lemmatized_output)))\n",
    "    #lemmatized_output = ' '.join([wordnet_lemmatizer.lemmatize(w) for w in words])\n",
    "    #print(\"lemma\",lemmatized_output)\n",
    "    return lemmatized_output\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                            lowercase=False,\n",
    "                            tokenizer= doc,\n",
    "                            #min_df = 10,\n",
    "                            preprocessor=regular_expression,\n",
    "                            #strip_accents =None,\n",
    "                            max_features = 200000, \n",
    "                            token_pattern = r'\\S+',\n",
    "                            ngram_range=(1,3)\n",
    "                            )\n",
    "\n",
    "training_features = vectorizer.fit_transform(X_train)\n",
    "\n",
    "#training_features = SelectKBest(chi2,k=250).fit_transform(training_features, Y_train)\n",
    "\n",
    "\n",
    "test_features = vectorizer.transform(X_test)\n",
    "#test_features = SelectKBest(chi2,k=250).fit_transform(test_features, Y_test)\n",
    "\n",
    "list_feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "print(training_features.shape)\n",
    "print(list_feature_name[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting\n",
      "done fitting\n",
      "0.2836991548610689\n"
     ]
    }
   ],
   "source": [
    "#classifier = OneVsRestClassifier(LogisticRegression(n_jobs=1, C=1))\n",
    "classifier = OneVsRestClassifier(LinearSVC())\n",
    "print(\"start fitting\")\n",
    "classifier.fit(training_features, Y_train)\n",
    "print(\"done fitting\")\n",
    "\n",
    "\n",
    "\n",
    "Y_test_pred = classifier.predict(test_features)\n",
    "\n",
    "print(accuracy_score(Y_test, Y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Read_from_test_set_unlabelled()\n",
    "df= pd.DataFrame({'test_text':test_data})\n",
    "test_real_features = vectorizer.transform(df['test_text'])\n",
    "Y_test_pred_output = classifier.predict(test_real_features)\n",
    "df = pd.DataFrame(Y_test_pred_output,columns=['Predicted'])\n",
    "df.index = df.index + 1\n",
    "df.index.name = 'Id'\n",
    "df.to_csv('predict_SML_Pro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              idf_weights\n",
      "                 1.339368\n",
      "                 1.938566\n",
      "                 3.880848\n",
      "  !             10.302410\n",
      "  #             10.132511\n",
      "  -              9.987329\n",
      "  .              5.877341\n",
      "  10            11.200351\n",
      "  12584         11.033297\n",
      "  140army       10.707875\n",
      "  140mafia      10.266042\n",
      "  2             10.890196\n",
      "  2009          10.379371\n",
      "  21bikes       10.340150\n",
      "  3              9.909367\n",
      "  301           11.200351\n",
      "  6             10.553724\n",
      "  7             11.113340\n",
      "  ?             10.101739\n",
      "  @              8.973274\n",
      "  A              9.471112\n",
      "  D             11.033297\n",
      "  HUFFPO         9.013279\n",
      "  I              7.666001\n",
      "  IHOP          10.707875\n",
      "  RT            10.420193\n",
      "  U             10.653808\n",
      "  act           11.033297\n",
      "  activerain    10.653808\n",
      "  ad             9.706426\n",
      "...                   ...\n",
      "“ ”             10.707875\n",
      "”                7.127447\n",
      "”                8.587611\n",
      "”                8.769933\n",
      "” -             10.890196\n",
      "” “             10.765033\n",
      "”.              10.959189\n",
      "•                9.307787\n",
      "•               11.200351\n",
      "…                7.717713\n",
      "…                9.228799\n",
      "…                9.471112\n",
      "… –             11.200351\n",
      "….              10.197049\n",
      "….              11.200351\n",
      "›               10.825658\n",
      "›               11.033297\n",
      "›               11.033297\n",
      "€                9.860577\n",
      "™               10.014728\n",
      "☺               10.825658\n",
      "♥                9.860577\n",
      "♥♫♪             10.890196\n",
      "♫                8.014750\n",
      "♫                8.164524\n",
      "♫                8.164524\n",
      "➔                9.109610\n",
      "➔                9.109610\n",
      "➔                9.109610\n",
      "。               10.602514\n",
      "\n",
      "[70233 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])\n",
    "print(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
